{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchinfo import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "no_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "acc_function = MulticlassAccuracy(num_classes=102, average='micro').to(device)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "generator = torch.Generator().manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Assuming you want to keep the default transformations for testing/validation:\n",
    "default_transforms = transforms.Compose([\n",
    "    models.ResNet34_Weights.IMAGENET1K_V1.transforms()\n",
    "])\n",
    "\n",
    "flowers_train = datasets.Flowers102(root='./data', split='train', download=True, transform=train_transforms)\n",
    "flowers_test = datasets.Flowers102(root='./data', split='test', download=True, transform=default_transforms)\n",
    "flowers_val = datasets.Flowers102(root='./data', split='val', download=True, transform=default_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(flowers_train, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    test_loader = torch.utils.data.DataLoader(flowers_test, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    val_loader = torch.utils.data.DataLoader(flowers_val, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    return train_loader, test_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping based on accuracy\n",
    "class AccuracyEarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.max_validation_accuracy = 0\n",
    "\n",
    "    def early_stop(self, validation_accuracy):\n",
    "        if validation_accuracy > (self.max_validation_accuracy + self.min_delta):\n",
    "            self.max_validation_accuracy = validation_accuracy\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, loss_fn):\n",
    "    running_loss_value = 0\n",
    "    for images, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        running_loss_value += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return running_loss_value / len(dataloader)\n",
    "\n",
    "def test_eval(model, dataloader, loss_fn):\n",
    "    running_loss_value = 0\n",
    "    running_acc_value = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            acc = acc_function(outputs, labels)\n",
    "            running_loss_value += loss.item()\n",
    "            running_acc_value += acc.item()\n",
    "    running_acc_value /= len(dataloader)\n",
    "    running_loss_value /= len(dataloader)\n",
    "    return running_acc_value*100, running_loss_value\n",
    "\n",
    "def train_eval_test(model, train_dataloader, val_dataloader, test_dataloader, loss_fn, no_epochs=10):\n",
    "    es = AccuracyEarlyStopper()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loss_arr, train_acc_arr, eval_loss_arr, eval_acc_arr = [], [], [], []\n",
    "    for i in range(no_epochs):\n",
    "        train_loss = train(model, optimizer, train_dataloader, loss_fn)\n",
    "        eval_acc, eval_loss = test_eval(model, val_dataloader)\n",
    "        print(f'Epoch {i+1} Train Loss: {train_loss:>8f}, Eval Accuracy: {eval_acc:>0.2f}%, Eval Loss: {eval_loss:>8f}')\n",
    "        train_loss_arr.append(train_loss)\n",
    "        eval_loss_arr.append(eval_loss)\n",
    "        eval_acc_arr.append(eval_acc)\n",
    "        if es.early_stop(eval_acc):\n",
    "            print('Early stopping activated')\n",
    "            break\n",
    "    test_acc, test_loss = test_eval(model, test_dataloader, loss_fn)\n",
    "    print(f\"Test Accuracy: {test_acc}, Test Loss: {test_loss}\")\n",
    "    return train_loss_arr, train_acc_arr, eval_loss_arr, eval_acc_arr, test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader, test_data_loader, val_data_loader = get_data_loader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(512, 102)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEL_smoothing = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Builds\\CZ4042-group-project\\loss.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m model \u001b[39m=\u001b[39m create_model()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m \u001b[39m# Training and evaluation\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m train_loss_arr, train_acc_arr, test_acc \u001b[39m=\u001b[39m train_eval_test(model, train_data_loader, val_data_loader, test_data_loader, no_epochs\u001b[39m=\u001b[39;49mno_epochs)\n",
      "\u001b[1;32mc:\\Builds\\CZ4042-group-project\\loss.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train(model, optimizer, triplet_train_loader, nn\u001b[39m.\u001b[39mTripletMarginLoss(margin\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m# Validation\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m eval_acc \u001b[39m=\u001b[39m test_eval(model, val_dataloader)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m>8f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Eval Accuracy: \u001b[39m\u001b[39m{\u001b[39;00meval_acc\u001b[39m:\u001b[39;00m\u001b[39m>0.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m train_loss_arr\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Builds\\CZ4042-group-project\\loss.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m running_acc_value \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     \u001b[39mfor\u001b[39;00m anchor, positive, negative \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m         anchor \u001b[39m=\u001b[39m anchor\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Builds/CZ4042-group-project/loss.ipynb#X12sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m         positive \u001b[39m=\u001b[39m positive\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import numpy as np\n",
    "\n",
    "no_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "acc_function = MulticlassAccuracy(num_classes=102, average='micro').to(device)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "# Data Augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Assuming you want to keep the default transformations for testing/validation:\n",
    "default_transforms = transforms.Compose([\n",
    "    models.ResNet34_Weights.IMAGENET1K_V1.transforms()\n",
    "])\n",
    "\n",
    "flowers_train = datasets.Flowers102(root='./data', split='train', download=True, transform=train_transforms)\n",
    "flowers_test = datasets.Flowers102(root='./data', split='test', download=True, transform=default_transforms)\n",
    "flowers_val = datasets.Flowers102(root='./data', split='val', download=True, transform=default_transforms)\n",
    "\n",
    "\n",
    "def get_data_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(flowers_train, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    test_loader = torch.utils.data.DataLoader(flowers_test, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    val_loader = torch.utils.data.DataLoader(flowers_val, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "# Custom dataloader for triplet loss\n",
    "class TripletDataLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor, anchor_label = self.dataset[index]\n",
    "        \n",
    "        # Select positive sample\n",
    "        positive_index = np.random.choice(np.where(np.array(self.dataset._labels) == anchor_label)[0])\n",
    "        positive, _ = self.dataset[positive_index]\n",
    "\n",
    "        # Select negative sample\n",
    "        negative_index = np.random.choice(np.where(np.array(self.dataset._labels) != anchor_label)[0])\n",
    "        negative, _ = self.dataset[negative_index]\n",
    "\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Early stopping based on accuracy\n",
    "class AccuracyEarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.max_validation_accuracy = 0\n",
    "\n",
    "    def early_stop(self, validation_accuracy):\n",
    "        if validation_accuracy > (self.max_validation_accuracy + self.min_delta):\n",
    "            self.max_validation_accuracy = validation_accuracy\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def create_model():\n",
    "    model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(512, 102)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def train(model, optimizer, dataloader, loss_fn):\n",
    "    running_loss_value = 0\n",
    "    for anchor, positive, negative in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        anchor_embedding = model(anchor)\n",
    "        positive_embedding = model(positive)\n",
    "        negative_embedding = model(negative)\n",
    "\n",
    "        # Compute triplet margin loss\n",
    "        loss = loss_fn(anchor_embedding, positive_embedding, negative_embedding)\n",
    "        \n",
    "        running_loss_value += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return running_loss_value / len(dataloader)\n",
    "\n",
    "def test_eval(model, dataloader):\n",
    "    running_acc_value = 0\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative in dataloader:\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_embedding = model(anchor)\n",
    "            positive_embedding = model(positive)\n",
    "            negative_embedding = model(negative)\n",
    "\n",
    "            # Compute triplet margin loss (just for monitoring, not for optimization)\n",
    "            loss_fn = nn.TripletMarginLoss(margin=1.0)\n",
    "            loss = loss_fn(anchor_embedding, positive_embedding, negative_embedding)\n",
    "\n",
    "            acc = acc_function(anchor_embedding, positive_embedding)\n",
    "            running_acc_value += acc.item()\n",
    "    running_acc_value /= len(dataloader)\n",
    "    return running_acc_value * 100\n",
    "\n",
    "def train_eval_test(model, train_dataloader, val_dataloader, test_dataloader, no_epochs=10):\n",
    "    es = AccuracyEarlyStopper()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loss_arr, train_acc_arr, eval_acc_arr = [], [], []\n",
    "    for i in range(no_epochs):\n",
    "        # Create custom triplet dataloaders for training\n",
    "        triplet_train_loader = torch.utils.data.DataLoader(TripletDataLoader(flowers_train), batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Training\n",
    "        train_loss = train(model, optimizer, triplet_train_loader, nn.TripletMarginLoss(margin=1.0).to(device))\n",
    "        \n",
    "        # Validation\n",
    "        eval_acc = test_eval(model, val_dataloader)\n",
    "        \n",
    "        print(f'Epoch {i + 1} Train Loss: {train_loss:>8f}, Eval Accuracy: {eval_acc:>0.2f}%')\n",
    "        \n",
    "        train_loss_arr.append(train_loss)\n",
    "        train_acc_arr.append(eval_acc)\n",
    "        \n",
    "        if es.early_stop(eval_acc):\n",
    "            print('Early stopping activated')\n",
    "            break\n",
    "    \n",
    "    # Testing\n",
    "    test_acc = test_eval(model, test_dataloader)\n",
    "    print(f\"Test Accuracy: {test_acc}\")\n",
    "    \n",
    "    return train_loss_arr, train_acc_arr, test_acc\n",
    "\n",
    "\n",
    "# Loaders\n",
    "train_data_loader, test_data_loader, val_data_loader = get_data_loader(batch_size)\n",
    "\n",
    "# Model\n",
    "model = create_model().to(device)\n",
    "\n",
    "# Training and evaluation\n",
    "train_loss_arr, train_acc_arr, test_acc = train_eval_test(model, train_data_loader, val_data_loader, test_data_loader, no_epochs=no_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
